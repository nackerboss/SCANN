{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nackerboss/SCANNN/blob/main/REAL_MAIN_ScaNN_Embedding_Search_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scann sentence-transformers datasets\n"
      ],
      "metadata": {
        "id": "w08aKpa29OBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "NrNu6kxo_qwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This script demonstrates the full workflow using a public dataset:\n",
        "# 1. Install necessary libraries (scann, sentence-transformers, datasets).\n",
        "# 2. Load a public text dataset (Hugging Face ag_news) for both training (index) and testing (queries).\n",
        "# 3. Generate and normalize vector embeddings for both sets.\n",
        "# 4. Build a high-performance ScaNN index on the training set.\n",
        "# 5. Build a Brute-Force searcher and compute recall using the test set queries.\n",
        "# 6. Run a sample similarity query.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Installation (Run this in a separate Colab cell first!) ---\n",
        "# Note: You now need 'datasets' installed.\n",
        "# !pip install scann sentence-transformers datasets\n",
        "\n",
        "try:\n",
        "    import scann\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from datasets import load_dataset # New import for public dataset\n",
        "except ImportError:\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    print(\"ðŸš¨ ERROR: Please run the following command in a separate Colab cell \")\n",
        "    print(\"and restart the runtime before running this code:\")\n",
        "    print(\"!pip install scann sentence-transformers datasets\")\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    exit()\n",
        "\n",
        "# --- Utility Function for Recall Calculation (Provided by user) ---\n",
        "\n",
        "def compute_recall(neighbors, true_neighbors):\n",
        "    \"\"\"\n",
        "    Computes recall @k by comparing the results of the approximate search\n",
        "    (neighbors) against the exact search (true_neighbors).\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    # Iterate through query results, comparing the approximate set against the true set\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        # Count the number of common elements (true positives)\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "\n",
        "    # Recall is (True Positives) / (Total True Neighbors)\n",
        "    return total / true_neighbors.size\n",
        "\n",
        "# --- 2. Setup and Data Loading ---\n",
        "\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "# Load a public dataset (ag_news) and take a manageable subset for demonstration\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "try:\n",
        "    # Load the training split (used for building the ScaNN index)\n",
        "    ag_news_dataset_train = load_dataset('ag_news', split='train[:5000]')\n",
        "    dataset = ag_news_dataset_train['text']\n",
        "\n",
        "    # Load the test split (used for generating test queries for recall calculation)\n",
        "    ag_news_dataset_test = load_dataset('ag_news', split='test[:20]')\n",
        "    test_dataset_text = ag_news_dataset_test['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset: {e}\")\n",
        "    # Fallback to the original small dataset if loading fails\n",
        "    dataset = [\n",
        "        \"The sun rises in the east every morning.\",\n",
        "        \"A computer uses a central processing unit for core tasks.\",\n",
        "        \"Cats and dogs are common household pets.\",\n",
        "        \"A feline companion enjoying a nap on the sofa.\",\n",
        "        \"The central processing unit is the brain of any modern machine.\",\n",
        "        \"Tomorrow's forecast predicts clear skies and warm weather.\"\n",
        "    ]\n",
        "    test_dataset_text = dataset # Use the same small data for queries if primary fails\n",
        "\n",
        "\n",
        "# The queries we will use to search the dataset\n",
        "query_text_1 = \"The main component of a PC is the CPU.\"\n",
        "query_text_2 = \"What is the weather like at dawn?\"\n",
        "query_text_3 = \"Football match results from the weekend.\"\n",
        "\n",
        "def generate_and_normalize(data):\n",
        "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(data)} items...\")\n",
        "\n",
        "    # 3.1 Generate embeddings (returns a numpy array)\n",
        "    embeddings = embedding_model.encode(\n",
        "        data,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # 3.2 L2 Normalization (Crucial for ScaNN dot product or angular similarity)\n",
        "    print(\"Normalizing embeddings...\")\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_embeddings, embeddings.shape[1]\n",
        "\n",
        "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
        "\n",
        "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
        "\n",
        "print(f\"\\nDataset Ready. Shape: {normalized_dataset_embeddings.shape}\")\n",
        "print(f\"Test Query Set Shape: {normalized_test_embeddings.shape}\")\n",
        "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
        "\n",
        "\n",
        "# --- 4. Building the ScaNN Index (Optimized for 5000 vectors) ---\n",
        "\n",
        "print(\"\\n--- 4. Building ScaNN Optimized Searcher (Trained on 5000 examples) ---\")\n",
        "\n",
        "# The maximum number of neighbors to retrieve (top-k)\n",
        "K_NEIGHBORS = 5\n",
        "REORDER_NEIGHBORS = 50 # Reduced reorder candidates for speedier demo\n",
        "\n",
        "# 4.1. Initialize the ScaNN builder\n",
        "# Arguments: (dataset, k, distance_metric)\n",
        "builder = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ")\n",
        "\n",
        "# 4.2. Configure the Tree (Partitioning) stage\n",
        "tree_configured = builder.tree(\n",
        "    num_leaves=500,\n",
        "    num_leaves_to_search=50,\n",
        "    training_sample_size=4000\n",
        ")\n",
        "\n",
        "# 4.3. Configure Asymmetric Hashing (AH) for scoring\n",
        "ah_configured = tree_configured.score_ah(\n",
        "    8, # Number of dimensions per subvector\n",
        "    anisotropic_quantization_threshold=0.2\n",
        ")\n",
        "\n",
        "# 4.4. Configure the Reordering (Refinement) stage\n",
        "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
        "\n",
        "# 4.5. Finalize and build the searcher\n",
        "searcher = reorder_configured.build()\n",
        "\n",
        "print(\"ScaNN optimized index built successfully.\")\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Ph879PTv9NK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Computing Recall (ScaNN vs. Brute Force) ---\n",
        "\n",
        "print(\"\\n--- 5. Computing Recall (ScaNN vs. Brute Force) ---\")\n",
        "\n",
        "# 5.1. Create a Brute-Force ScaNN searcher (no tree, no quantization)\n",
        "# This will find the mathematically exact nearest neighbors.\n",
        "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ").score_brute_force().build()\n",
        "\n",
        "# 5.2. Define Test Queries (using a subset of the official test split as queries)\n",
        "# Limit the number of test queries for faster recall computation\n",
        "MAX_TEST_QUERIES = 500\n",
        "NUM_RECALL_QUERIES = min(MAX_TEST_QUERIES, len(normalized_test_embeddings))\n",
        "\n",
        "# Use the dedicated test set embeddings for recall calculation\n",
        "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
        "\n",
        "print(f\"1. Running Brute-Force search on {NUM_RECALL_QUERIES} test queries...\")\n",
        "# .search_batched() is much faster for multiple queries\n",
        "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries)\n",
        "\n",
        "print(\"2. Running Optimized ScaNN search...\")\n",
        "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
        "\n",
        "# 5.3. Calculate and Print Recall\n",
        "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
        "print(f\"\\nâœ… Recall @{K_NEIGHBORS} for {NUM_RECALL_QUERIES} queries from the TEST split: {recall_value * 100:.2f}%\")\n",
        "print(\"This value indicates the percentage of exact nearest neighbors found by the approximate searcher.\")\n",
        "\n",
        "\n",
        "# --- 6. Running a Sample Query ---\n",
        "\n",
        "def run_query(query, search_index, original_dataset):\n",
        "    \"\"\"Embeds a query, normalizes it, and searches the ScaNN index.\"\"\"\n",
        "    print(f\"\\nSearching with query: '{query}'\")\n",
        "\n",
        "    # 6.1 Embed and Normalize the query\n",
        "    query_embedding = embedding_model.encode([query])[0]\n",
        "    normalized_query = query_embedding / np.linalg.norm(query_embedding)\n",
        "\n",
        "    # 6.2 Perform the search\n",
        "    # The 'k' parameter is configured during the builder step, so we omit it here.\n",
        "    indices, distances = search_index.search(normalized_query)\n",
        "\n",
        "    print(f\"\\nTop {len(indices)} results found:\")\n",
        "    for rank, (idx, distance) in enumerate(zip(indices, distances)):\n",
        "        print(f\"  Rank {rank+1}:\")\n",
        "        print(idx)\n",
        "        print(f\"    Text: {original_dataset[idx.item() ]}\")\n",
        "        # Dot product distance is 1.0 for perfect match, 0.0 for orthogonal\n",
        "        print(f\"    Similarity (Dot Product): {distance:.4f}\")\n",
        "        print(f\"    Dataset Index: {idx}\")\n",
        "\n",
        "# Run Query 1: Find sentences about computers\n",
        "run_query(query_text_1, searcher, dataset)\n",
        "\n",
        "# Run Query 2: Find sentences about weather/time\n",
        "run_query(query_text_2, searcher, dataset)\n",
        "\n",
        "# Run Query 3: Find relevant news articles\n",
        "run_query(query_text_3, searcher, dataset)"
      ],
      "metadata": {
        "id": "P-IYIPbMCrdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking Section\n",
        "\n",
        "This second section attempts to run both the built-in brute force algorithm of ScaNN and the actual algorithm in a larger scale."
      ],
      "metadata": {
        "id": "d5djiaS-qPjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------- development script used for generating an embedded! -----------\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Installation (Run this in a separate Colab cell first!) ---\n",
        "# Note: You now need 'datasets' installed.\n",
        "# !pip install scann sentence-transformers datasets\n",
        "\n",
        "try:\n",
        "    import scann\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from datasets import load_dataset # New import for public dataset\n",
        "except ImportError:\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    print(\"ðŸš¨ ERROR: Please run the following command in a separate Colab cell \")\n",
        "    print(\"and restart the runtime before running this code:\")\n",
        "    print(\"!pip install scann sentence-transformers datasets\")\n",
        "    print(\"----------------------------------------------------------------------\")\n",
        "    exit()\n",
        "\n",
        "print(\"This is a Colab script to load the dataset, embed, then save it into agnews_embeddings.h5.\")\n",
        "\n",
        "num_headlines = int(input(\"Enter the number of news headlines to convert, normalize, and be saved to the embeddings file.\"))\n",
        "\n",
        "if (num_headlines > 120000) or (num_headlines < 1):\n",
        "  print('Invalid input. num_headlines is set back to 5000.')\n",
        "  num_headlines = 5000\n",
        "\n",
        "# 2.\n",
        "\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "embedding_model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "try:\n",
        "    ag_news_dataset_train = load_dataset('ag_news', split=f'train[:{num_headlines}]') # Loads dataset with num_training entries\n",
        "    dataset = ag_news_dataset_train['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset. {e}. Since this is only used for benchmarking, the hardcoded dataset is ignored and the program is cancelled.\")\n",
        "\n",
        "#------- Inner function, declared right before use -------\n",
        "def generate_and_normalize(data):\n",
        "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(data)} items...\")\n",
        "\n",
        "    embeddings = embedding_model.encode(\n",
        "        data,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Normalizing embeddings...\")\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_embeddings, embeddings.shape[1]\n",
        "#------- Inner function, declared right before use -------\n",
        "\n",
        "normalized_dataset_embeddings, embedding_dim = generate_and_normalize(dataset)\n",
        "\n",
        "print(f\"\\nDataset generated: {normalized_dataset_embeddings.shape}\")\n",
        "print(f\"First dataset entry (Index Training Data): {dataset[0]}\")\n",
        "\n",
        "# This script has finished generating and normalizing. The next cell saves them.\n",
        "\n"
      ],
      "metadata": {
        "id": "_noI77dxOHEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "filename = 'agnews_embeddings.h5'\n",
        "dataset_name = 'agnews'\n",
        "\n",
        "# 2. Save the embeddings to the H5 file\n",
        "try:\n",
        "    with h5py.File(filename, 'w') as f:\n",
        "        # Create a dataset to hold the embedding array\n",
        "        dset = f.create_dataset(dataset_name, data=normalized_dataset_embeddings)\n",
        "\n",
        "        # Optionally, you can add metadata as attributes\n",
        "        dset.attrs['description'] = 'Embeddings for my project'\n",
        "        dset.attrs['dimension'] = normalized_dataset_embeddings.shape[1]\n",
        "\n",
        "    print(f\"Embeddings successfully saved to {filename} under the dataset '{dataset_name}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "NYLs5nN66rUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark section\n",
        "This used a pregen'd embedded dataset from a file (likely generated from the earlier snippets). Run from this point onwards to see time results."
      ],
      "metadata": {
        "id": "pRt0vXiZh3Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To load the embeddings back\n",
        "import h5py\n",
        "\n",
        "filename = 'agnews_embeddings.h5'\n",
        "dataset_name = 'agnews'\n",
        "\n",
        "try:\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        # Access the dataset\n",
        "        loaded_embeddings = f[dataset_name][:]\n",
        "\n",
        "        print(\"\\nEmbeddings loaded successfully.\")\n",
        "        print(\"Shape:\", loaded_embeddings.shape)\n",
        "        print(\"Metadata description:\", f[dataset_name].attrs['description'])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during loading: {e}\")"
      ],
      "metadata": {
        "id": "Z4wU2bkSMu3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "normalized_dataset_embeddings = loaded_embeddings\n",
        "num_headlines = loaded_embeddings.shape[0]\n",
        "\n",
        "K_NEIGHBORS = input(\"\\nEnter k (the number of nearest neighbors to find): \")\n",
        "try:\n",
        "    K_NEIGHBORS = int(K_NEIGHBORS)\n",
        "    if K_NEIGHBORS < 1 or K_NEIGHBORS > 100:\n",
        "        print(\"Invalid k. Setting to default: 5\")\n",
        "        K_NEIGHBORS = 5\n",
        "except ValueError:\n",
        "    print(\"Invalid k. Setting to default: 5\")\n",
        "    K_NEIGHBORS = 5\n",
        "\n",
        "REORDER_NEIGHBORS = input(\"Enter the number of reorder candidates (recommended: 10*k): \")\n",
        "try:\n",
        "    REORDER_NEIGHBORS = int(REORDER_NEIGHBORS)\n",
        "    if REORDER_NEIGHBORS < K_NEIGHBORS:\n",
        "        print(f\"Reorder candidates must be >= k. Setting to {K_NEIGHBORS * 10}\")\n",
        "        REORDER_NEIGHBORS = K_NEIGHBORS * 10\n",
        "except ValueError:\n",
        "    print(f\"Invalid input. Setting to {K_NEIGHBORS * 10}\")\n",
        "    REORDER_NEIGHBORS = K_NEIGHBORS * 10\n",
        "\n",
        "# ------ Section: Normalized TEST Embeddings ----\n",
        "\n",
        "num_tests = int(input(\"Enter the number of test queries to generate: \"))\n",
        "if (num_tests > 1000) or (num_tests < 1):\n",
        "  print('Invalid input. num_tests is set back to 100.')\n",
        "  num_tests = 100\n",
        "\n",
        "print(\"Loading public dataset (ag_news) subset...\")\n",
        "\n",
        "try:\n",
        "    # Load the test split (used for generating test queries for recall calculation)\n",
        "    ag_news_dataset_test = load_dataset('ag_news', split=f'test[:{num_tests}]')\n",
        "    test_dataset_text = ag_news_dataset_test['text']\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading ag_news dataset: {e}\")\n",
        "\n",
        "#------- Inner function, declared right before use -------\n",
        "def generate_and_normalize(data):\n",
        "    \"\"\"Generates embeddings and performs L2 normalization.\"\"\"\n",
        "    print(f\"Generating embeddings for {len(data)} items...\")\n",
        "\n",
        "    embeddings = embedding_model.encode(\n",
        "        data,\n",
        "        convert_to_tensor=False,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    print(\"Normalizing embeddings...\")\n",
        "    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    return normalized_embeddings, embeddings.shape[1]\n",
        "#------- Inner function, declared right before use -------\n",
        "\n",
        "normalized_test_embeddings, _ = generate_and_normalize(test_dataset_text)\n",
        "\n",
        "# --- 5. Dynamic ScaNN Parameters Based on Dataset Size ---\n",
        "print(\"\\n--- Building ScaNN Index with Dynamic Parameters ---\")\n",
        "\n",
        "# Calculate optimal parameters based on dataset size\n",
        "num_leaves = max(int(np.sqrt(num_headlines)), 100)  # rcm val: sqrt(num_hl);\n",
        "num_leaves_to_search = max(int(num_leaves * 0.1), 10)  # can't know for sure, requires tuning, will add prompt to enter this number later\n",
        "training_sample_size = min(int(num_headlines * 0.69), num_headlines - 1)  # 80% of dataset, prevents overfitting, fits for smaller dataset\n",
        "\n",
        "print(f\"Dataset size: {num_headlines}\")\n",
        "print(f\"Number of leaves (clusters): {num_leaves}\")\n",
        "print(f\"Leaves to search: {num_leaves_to_search}\")\n",
        "print(f\"Training sample size: {training_sample_size}\")\n",
        "print(f\"K neighbors: {K_NEIGHBORS}\")\n",
        "print(f\"Reorder candidates: {REORDER_NEIGHBORS}\")\n",
        "\n",
        "# --- 6. Build ScaNN Index ---\n",
        "builder = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ")\n",
        "\n",
        "tree_configured = builder.tree(\n",
        "    num_leaves=num_leaves,\n",
        "    num_leaves_to_search=num_leaves_to_search,\n",
        "    training_sample_size=training_sample_size\n",
        ")\n",
        "\n",
        "ah_configured = tree_configured.score_ah(\n",
        "    8,  # Number of dimensions per subvector\n",
        "    anisotropic_quantization_threshold=0.2\n",
        ")\n",
        "\n",
        "reorder_configured = ah_configured.reorder(REORDER_NEIGHBORS)\n",
        "searcher = reorder_configured.build()\n",
        "\n",
        "print(\"ScaNN optimized index built successfully.\")\n",
        "\n",
        "# -----------\n",
        "\n",
        "print(\"\\n--- Computing Recall (ScaNN vs. Brute Force) ---\")\n",
        "\n",
        "def compute_recall(neighbors, true_neighbors):\n",
        "    \"\"\"Computes recall @k.\"\"\"\n",
        "    total = 0\n",
        "    for gt_row, row in zip(true_neighbors, neighbors):\n",
        "        total += np.intersect1d(gt_row, row).shape[0]\n",
        "    return total / true_neighbors.size\n",
        "\n",
        "# Build brute-force searcher\n",
        "bruteforce_searcher = scann.scann_ops_pybind.builder(\n",
        "    normalized_dataset_embeddings,\n",
        "    K_NEIGHBORS,\n",
        "    \"dot_product\"\n",
        ").score_brute_force().build()\n",
        "\n",
        "test_query_input = input(\"\\nEnter number of test queries for recall evaluation (or 'all' for complete test): \")\n",
        "\n",
        "if test_query_input.lower() == 'all':\n",
        "    NUM_RECALL_QUERIES = len(normalized_test_embeddings)\n",
        "    print(f\"Testing on ALL {NUM_RECALL_QUERIES} test queries (most accurate, takes longer)\")\n",
        "else:\n",
        "    try:\n",
        "        requested_queries = int(test_query_input)\n",
        "        NUM_RECALL_QUERIES = min(requested_queries, len(normalized_test_embeddings))\n",
        "        print(f\"Testing on {NUM_RECALL_QUERIES} test queries\")\n",
        "    except ValueError:\n",
        "        NUM_RECALL_QUERIES = min(1000, len(normalized_test_embeddings))\n",
        "        print(f\"Invalid input. Using default: {NUM_RECALL_QUERIES} test queries\")\n",
        "\n",
        "recall_test_queries = normalized_test_embeddings[:NUM_RECALL_QUERIES]\n",
        "\n",
        "brute_force_time_start = time.perf_counter()\n",
        "\n",
        "true_neighbors, _ = bruteforce_searcher.search_batched(recall_test_queries) # Brute-force searches\n",
        "\n",
        "brute_force_time_end = time.perf_counter()\n",
        "\n",
        "scann_time_start = time.perf_counter()\n",
        "\n",
        "print(\"Running Optimized ScaNN search...\")\n",
        "scann_neighbors, _ = searcher.search_batched(recall_test_queries)\n",
        "\n",
        "scann_time_end = time.perf_counter()\n",
        "\n",
        "recall_value = compute_recall(scann_neighbors, true_neighbors)\n",
        "print(f\"\\nRecall @{K_NEIGHBORS}: {recall_value * 100:.2f}%\")\n",
        "print(\"(Percentage of exact nearest neighbors found by ScaNN)\")\n",
        "print(f\"Done. Brute-force time: {brute_force_time_end - brute_force_time_start}\")\n",
        "print(f\"ScaNN time: {scann_time_end - scann_time_start}\")"
      ],
      "metadata": {
        "id": "fGYRopi-uMv5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}